{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "DATASET_OPTIONS = [\"steel\", \"fetal_health\"]\n",
    "DATASET = \"fetal_health\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"steel\":\n",
    "    if not os.path.exists('data/steel'):\n",
    "        os.makedirs('data/steel')\n",
    "    if not os.path.exists('data/steel/steel.train'):\n",
    "        # fetch dataset\n",
    "        steel_plates_faults = fetch_ucirepo(id=198)\n",
    "\n",
    "        # data (as pandas dataframes)\n",
    "        X = steel_plates_faults.data.features\n",
    "        y = steel_plates_faults.data.targets\n",
    "\n",
    "        # preprocess the targets\n",
    "        # put the class as the index of the 1 in the row\n",
    "        y = y.idxmax(axis=1)\n",
    "\n",
    "        # convert to numbers\n",
    "        y = y.astype('category').cat.codes\n",
    "\n",
    "        # split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        # saving the data\n",
    "        X_train.to_csv('data/steel/steel.train', index=False)\n",
    "        X_test.to_csv('data/steel/steel.test', index=False)\n",
    "        y_train.to_csv('data/steel/steel.train.target', index=False)\n",
    "        y_test.to_csv('data/steel/steel.test.target', index=False)\n",
    "\n",
    "        print('Data saved in data/steel/steel.train, data/steel/steel.test, data/steel/steel.train.target, data/steel/steel.test.target')\n",
    "    else:\n",
    "        print('Data already exists in data/steel/steel.train, data/steel/steel.test, data/steel/steel.train.target, data/steel/steel.test.target')\n",
    "\n",
    "    # load the data\n",
    "    X_train = pd.read_csv('data/steel/steel.train')\n",
    "    X_test = pd.read_csv('data/steel/steel.test')\n",
    "    y_train = pd.read_csv('data/steel/steel.train.target')\n",
    "    y_test = pd.read_csv('data/steel/steel.test.target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists in data/fetal_health/fetal_health.train, data/fetal_health/fetal_health.test, data/fetal_health/fetal_health.train.target, data/fetal_health/fetal_health.test.target\n"
     ]
    }
   ],
   "source": [
    "if DATASET == \"fetal_health\":\n",
    "    if not os.path.exists('data/fetal_health'):\n",
    "        os.makedirs('data/fetal_health')\n",
    "\n",
    "    if not os.path.exists('data/fetal_health/fetal_health.train'):\n",
    "        # this data is meant to have been downloaded from kaggle\n",
    "        fetal_health = pd.read_csv('data/fetal_health/fetal_health.csv')\n",
    "\n",
    "        # split data\n",
    "        X = fetal_health.drop(columns='fetal_health')\n",
    "        y = fetal_health['fetal_health']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        # saving the data\n",
    "        X_train.to_csv('data/fetal_health/fetal_health.train', index=False)\n",
    "        X_test.to_csv('data/fetal_health/fetal_health.test', index=False)\n",
    "        y_train.to_csv('data/fetal_health/fetal_health.train.target', index=False)\n",
    "        y_test.to_csv('data/fetal_health/fetal_health.test.target', index=False)\n",
    "\n",
    "        print('Data saved in data/fetal_health/fetal_health.train, data/fetal_health/fetal_health.test, data/fetal_health/fetal_health.train.target, data/fetal_health/fetal_health.test.target')\n",
    "    else:\n",
    "        print('Data already exists in data/fetal_health/fetal_health.train, data/fetal_health/fetal_health.test, data/fetal_health/fetal_health.train.target, data/fetal_health/fetal_health.test.target')\n",
    "\n",
    "    # load the data\n",
    "    X_train = pd.read_csv('data/fetal_health/fetal_health.train')\n",
    "    X_test = pd.read_csv('data/fetal_health/fetal_health.test')\n",
    "    y_train = pd.read_csv('data/fetal_health/fetal_health.train.target')\n",
    "    y_test = pd.read_csv('data/fetal_health/fetal_health.test.target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: fetal_health\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using dataset: {DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {DEVICE} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "def normalize(X):\n",
    "    return (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "# extracting a validation set\n",
    "VALIDATION_SIZE = 0.1\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATION_SIZE)\n",
    "\n",
    "# creating the tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_val = torch.tensor(y_val, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# send to device\n",
    "X_train = X_train.to(DEVICE)\n",
    "X_val = X_val.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "\n",
    "y_train = y_train.to(DEVICE)\n",
    "y_val = y_val.to(DEVICE)\n",
    "y_test = y_test.to(DEVICE)\n",
    "\n",
    "# create the dataset\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with dropout and batch normalization\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        A Multi-Layer Perceptron model with 3 hidden layers of 256, 128, and 64 neurons each\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        A Multi-Layer Perceptron model with 5 hidden layers of 512, 256, 128, 64, 32 neurons each\n",
    "        \"\"\"\n",
    "        super(DeepMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        A Convolutional Neural Network model with 2 convolutional layers and 2 fully connected layers\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * (input_size // 4), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.model(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "INPUT_SIZE = X_train.shape[1]\n",
    "OUTPUT_SIZE = y_train.max() + 1\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 1000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 Train loss: 1.1924 Val loss: 1.1885 Test loss: 1.1783\n",
      "Epoch 2/1000 Train loss: 0.9934 Val loss: 0.9888 Test loss: 0.9699\n",
      "Epoch 3/1000 Train loss: 0.9759 Val loss: 0.9721 Test loss: 0.9513\n",
      "Epoch 4/1000 Train loss: 0.9730 Val loss: 0.9695 Test loss: 0.9483\n",
      "Epoch 5/1000 Train loss: 0.9720 Val loss: 0.9685 Test loss: 0.9472\n",
      "Epoch 6/1000 Train loss: 0.9715 Val loss: 0.9681 Test loss: 0.9466\n",
      "Epoch 7/1000 Train loss: 0.9712 Val loss: 0.9678 Test loss: 0.9463\n",
      "Epoch 8/1000 Train loss: 0.9710 Val loss: 0.9676 Test loss: 0.9461\n",
      "Epoch 9/1000 Train loss: 0.9709 Val loss: 0.9675 Test loss: 0.9460\n",
      "Epoch 10/1000 Train loss: 0.9708 Val loss: 0.9675 Test loss: 0.9459\n",
      "Epoch 11/1000 Train loss: 0.9707 Val loss: 0.9674 Test loss: 0.9458\n",
      "Epoch 12/1000 Train loss: 0.9707 Val loss: 0.9674 Test loss: 0.9458\n",
      "Epoch 13/1000 Train loss: 0.9706 Val loss: 0.9673 Test loss: 0.9457\n",
      "Epoch 14/1000 Train loss: 0.9706 Val loss: 0.9673 Test loss: 0.9457\n",
      "Epoch 15/1000 Train loss: 0.9705 Val loss: 0.9673 Test loss: 0.9456\n",
      "Epoch 16/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 17/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 18/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 19/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 20/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 21/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 22/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 23/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 24/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 25/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 26/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 27/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 28/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 29/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 30/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 31/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 32/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 33/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9456\n",
      "Epoch 34/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 35/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 36/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 37/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 38/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 39/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 40/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 41/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 42/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 43/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 44/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 45/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 46/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 47/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 48/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 49/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 50/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 51/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 52/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 53/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 54/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 55/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 56/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 57/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 58/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 59/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 60/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 61/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 62/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 63/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 64/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 65/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 66/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 67/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 68/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 69/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 70/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 71/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 72/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 73/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 74/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 75/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 76/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 77/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 78/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 79/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 80/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 81/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 82/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 83/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 84/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 85/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 86/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 87/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 88/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 89/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 90/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 91/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 92/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 93/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 94/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 95/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 96/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 97/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 98/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 99/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 100/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 101/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 102/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 103/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 104/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 105/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 106/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 107/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 108/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 109/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 110/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 111/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 112/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 113/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 114/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 115/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 116/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 117/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 118/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 119/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 120/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 121/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 122/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 123/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 124/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 125/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 126/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 127/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 128/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 129/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 130/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 131/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 132/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 133/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 134/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 135/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 136/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 137/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 138/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 139/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 140/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 141/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 142/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 143/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 144/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 145/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 146/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 147/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 148/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 149/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 150/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 151/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 152/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 153/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 154/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 155/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 156/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 157/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 158/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 159/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 160/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 161/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 162/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 163/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 164/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 165/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 166/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 167/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 168/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 169/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 170/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 171/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 172/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 173/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 174/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 175/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 176/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 177/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 178/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 179/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 180/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 181/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 182/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 183/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 184/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 185/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 186/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 187/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 188/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 189/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 190/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 191/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 192/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 193/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 194/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 195/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 196/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 197/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 198/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 199/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 200/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 201/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 202/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 203/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 204/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 205/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 206/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 207/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 208/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 209/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 210/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 211/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 212/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 213/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 214/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 215/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 216/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 217/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 218/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 219/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 220/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 221/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 222/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 223/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 224/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 225/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 226/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 227/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 228/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Epoch 229/1000 Train loss: 0.9705 Val loss: 0.9672 Test loss: 0.9455\n",
      "Early stopping at epoch 229\n",
      "Best epoch: 129 with val loss: 0.9672\n"
     ]
    }
   ],
   "source": [
    "# set the seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# training the model\n",
    "model = MLP(INPUT_SIZE, OUTPUT_SIZE).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 100\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # scheduler.step(loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = criterion(model(X_train), y_train)\n",
    "        val_loss = criterion(model(X_val), y_val)\n",
    "        test_loss = criterion(model(X_test), y_test)\n",
    "        training_losses.append(float(train_loss))\n",
    "        validation_losses.append(float(val_loss))\n",
    "        \n",
    "        print(f'Epoch {epoch}/{EPOCHS} Train loss: {train_loss:.4f} Val loss: {val_loss:.4f} Test loss: {test_loss:.4f}')\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                print(f'Best epoch: {best_epoch} with val loss: {best_val_loss:.4f}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graph the losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(training_losses, label='Training loss')\n",
    "plt.plot(validation_losses, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      1.00      0.89       340\n",
      "           2       0.00      0.00      0.00        53\n",
      "           3       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.80       426\n",
      "   macro avg       0.27      0.33      0.30       426\n",
      "weighted avg       0.64      0.80      0.71       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).argmax(dim=1)\n",
    "    print(classification_report(y_test.cpu(), y_pred.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, recall, and f1 score (the macro average)\n",
    "\n",
    "    Args:\n",
    "    y_true: true labels\n",
    "    y_pred: predicted labels\n",
    "\n",
    "    Returns:\n",
    "    a dictionary with the computed metrics\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    out = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_specs, data_specs, epochs, early_stopping_patience=100, seed=0, logging=True, metric_specs=None, return_model=False):\n",
    "    \"\"\"\n",
    "    Regular training loop with early stopping\n",
    "\n",
    "    model_specs: dict\n",
    "        model: torch.nn.Module\n",
    "        criterion: torch.nn.Module\n",
    "        optimizer: torch.optim.Optimizer\n",
    "        scheduler: torch.optim.lr_scheduler\n",
    "        train_loader: torch.utils.data.DataLoader\n",
    "        val_loader: torch.utils.data.DataLoader\n",
    "    data_specs: dict\n",
    "        X_train: torch.Tensor\n",
    "        y_train: torch.Tensor\n",
    "        X_val: torch.Tensor\n",
    "        y_val: torch.Tensor\n",
    "    epochs: int\n",
    "        Number of epochs to train the model\n",
    "    early_stopping_patience: int\n",
    "        Number of epochs to wait before early stopping when the key metric does not improve\n",
    "    seed: int\n",
    "        Random seed\n",
    "    logging: bool\n",
    "        Whether to print the training and validation losses\n",
    "    metric_specs: dict\n",
    "        metrics: function\n",
    "            A function that takes y_true and y_pred and returns a dictionary of metrics\n",
    "        larger_is_better: bool\n",
    "            Whether a larger value of the key metric is better\n",
    "        key_metric: str\n",
    "            The key metric to use for early stopping\n",
    "    return_model: bool\n",
    "        Whether to return the trained model\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    model = model_specs['model']\n",
    "    criterion = model_specs['criterion']\n",
    "    optimizer = model_specs['optimizer']\n",
    "    scheduler = model_specs['scheduler']\n",
    "    train_loader = model_specs['train_loader']\n",
    "    val_loader = model_specs['val_loader']\n",
    "\n",
    "    X_train = data_specs['X_train']\n",
    "    y_train = data_specs['y_train']\n",
    "    X_val = data_specs['X_val']\n",
    "    y_val = data_specs['y_val']\n",
    "\n",
    "    if metric_specs is not None:\n",
    "        get_metrics = metric_specs['metrics']\n",
    "        larger_is_better = metric_specs['larger_is_better']\n",
    "        key_metric = metric_specs['key_metric']\n",
    "    else:\n",
    "        get_metrics = None\n",
    "        larger_is_better = False\n",
    "        key_metric = None\n",
    "\n",
    "\n",
    "    best_metric = float('-inf') if larger_is_better else float('inf')\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    training_key_metrics = []\n",
    "    validation_key_metrics = []\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_loss = criterion(model(X_train), y_train)\n",
    "            val_loss = criterion(model(X_val), y_val)\n",
    "            training_losses.append(float(train_loss))\n",
    "            validation_losses.append(float(val_loss))\n",
    "\n",
    "            if metric_specs is not None:\n",
    "                y_train_pred = model(X_train)\n",
    "                y_val_pred = model(X_val)\n",
    "                # evaluate the metrics on cpu\n",
    "                metrics_train = get_metrics(y_train.cpu().numpy(), y_train_pred.cpu().argmax(dim=1).numpy())\n",
    "                metrics_val = get_metrics(y_val.cpu().numpy(), y_val_pred.cpu().argmax(dim=1).numpy())\n",
    "            \n",
    "            if logging and epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}/{epochs} Train loss: {train_loss:.4f} Val loss: {val_loss:.4f} ', end='')\n",
    "                if metric_specs is not None:\n",
    "                      print(f'Train {key_metric}: {metrics_train[key_metric]:.4f} Val {key_metric}: {metrics_val[key_metric]:.4f}')\n",
    "                else:\n",
    "                    print('')\n",
    "\n",
    "            # Check for early stopping\n",
    "            if metric_specs is not None:\n",
    "                if larger_is_better:\n",
    "                    val_metric = metrics_val[key_metric]\n",
    "                    if val_metric > best_metric:\n",
    "                        best_metric = val_metric\n",
    "                        best_epoch = epoch\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= early_stopping_patience:\n",
    "                            if logging:\n",
    "                                print(f'Early stopping at epoch {epoch}')\n",
    "                                print(f'Best epoch: {best_epoch} with val {key_metric}: {best_metric:.4f}')\n",
    "                            break\n",
    "                else:\n",
    "                    val_metric = metrics_val[key_metric]\n",
    "                    if val_metric < best_metric:\n",
    "                        best_metric = val_metric\n",
    "                        best_epoch = epoch\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= early_stopping_patience:\n",
    "                            if logging:\n",
    "                                print(f'Early stopping at epoch {epoch}')\n",
    "                                print(f'Best epoch: {best_epoch} with val {key_metric}: {best_metric:.4f}')\n",
    "                            break\n",
    "            else:\n",
    "                # loss\n",
    "                if val_loss < best_metric:\n",
    "                    best_metric = val_loss\n",
    "                    best_epoch = epoch\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= early_stopping_patience:\n",
    "                        if logging:\n",
    "                            print(f'Early stopping at epoch {epoch}')\n",
    "                            print(f'Best epoch: {best_epoch} with val loss: {best_metric:.4f}')\n",
    "                        break\n",
    "\n",
    "    return model if return_model else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from results.csv\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "from itertools import product\n",
    "\n",
    "# this grid search is has been run before and the findings were that\n",
    "# 1. Adam is better than SGD for this dataset\n",
    "# 2. StepLR is better than ReduceLROnPlateau\n",
    "# 3. 32 is the batch size is not that good\n",
    "\n",
    "# the original grid search\n",
    "# MODELS = [MLP, DeepMLP, CNN]\n",
    "# SCHEDULERS = [None, optim.lr_scheduler.ReduceLROnPlateau, optim.lr_scheduler.StepLR]\n",
    "# LEARNING_RATES = [0.0001, 0.001, 0.01]\n",
    "# BATCH_SIZES = [32, 64, 128]\n",
    "# OPTIMIZERS = [optim.Adam, optim.SGD]\n",
    "# EPOCHS = 2000\n",
    "\n",
    "# reduced grid search\n",
    "MODELS = [MLP, DeepMLP, CNN]\n",
    "SCHEDULERS = [None, optim.lr_scheduler.StepLR]\n",
    "LEARNING_RATES = [0.0001, 0.001, 0.01]\n",
    "BATCH_SIZES = [64, 128]\n",
    "OPTIMIZERS = [optim.Adam]\n",
    "EPOCHS = 2000\n",
    "\n",
    "# all the combinations of the hyperparameters\n",
    "combinations = list(product(MODELS, OPTIMIZERS, LEARNING_RATES, BATCH_SIZES, SCHEDULERS))\n",
    "\n",
    "\n",
    "# metrics\n",
    "metric_specs = {\n",
    "    'metrics': compute_metrics,\n",
    "    'larger_is_better': True,\n",
    "    'key_metric': 'f1'\n",
    "}\n",
    "\n",
    "# training the models with the grid search\n",
    "results = []\n",
    "\n",
    "# try to read the results from the file\n",
    "if os.path.exists('results.csv'):\n",
    "    results = pd.read_csv('results.csv', index_col=0).to_dict(orient='records')\n",
    "    print('Results loaded from results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All combinations have been trained\n",
      "Total time taken in minutes: 0.00m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "START = time.time()\n",
    "while True:\n",
    "    if len(results) >= len(combinations):\n",
    "        print('All combinations have been trained')\n",
    "        break\n",
    "\n",
    "    i = len(results)\n",
    "    model, optimizer, lr, batch_size, scheduler = combinations[i]\n",
    "    print(f'Combination {i + 1}/{len(combinations)}')\n",
    "    print(f'lr: {lr} batch_size: {batch_size} scheduler: {scheduler} optimizer: {optimizer} model: {model.__name__}')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = model(INPUT_SIZE, OUTPUT_SIZE).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer(model.parameters(), lr=lr)\n",
    "    if scheduler is not None:\n",
    "        if scheduler == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "            scheduler = scheduler(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "        else:\n",
    "            scheduler = scheduler(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    model_specs = {\n",
    "        'model': model,\n",
    "        'criterion': criterion,\n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader\n",
    "    }\n",
    "\n",
    "    data_specs = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val\n",
    "    }\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    trained_model = train_model(model_specs, data_specs, EPOCHS, metric_specs=metric_specs, logging=False, return_model=True)\n",
    "    if trained_model is not None:\n",
    "        trained_model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = trained_model(X_test)\n",
    "            metrics_test = compute_metrics(y_test.cpu().numpy(), y_test_pred.cpu().argmax(dim=1).numpy())\n",
    "            results.append(metrics_test)\n",
    "    \n",
    "    print(f'Time taken: {time.time() - start:.2f}s')\n",
    "\n",
    "print(f'Total time taken in minutes: {(time.time() - START) / 60:.2f}m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         precision    recall        f1\n",
      "model                                 \n",
      "CNN       0.266041  0.333333  0.295909\n",
      "DeepMLP   0.832020  0.848236  0.835378\n",
      "MLP       0.855347  0.853191  0.850109\n",
      "Top 5 MLP configurations\n",
      "(<class '__main__.MLP'>, <class 'torch.optim.adam.Adam'>, 0.001, 128, <class 'torch.optim.lr_scheduler.StepLR'>)\n",
      "(<class '__main__.MLP'>, <class 'torch.optim.adam.Adam'>, 0.01, 128, None)\n",
      "(<class '__main__.MLP'>, <class 'torch.optim.adam.Adam'>, 0.01, 128, <class 'torch.optim.lr_scheduler.StepLR'>)\n",
      "(<class '__main__.MLP'>, <class 'torch.optim.adam.Adam'>, 0.001, 64, None)\n",
      "(<class '__main__.MLP'>, <class 'torch.optim.adam.Adam'>, 0.0001, 64, None)\n",
      "Top 5 DeepMLP configurations\n",
      "(<class '__main__.DeepMLP'>, <class 'torch.optim.adam.Adam'>, 0.001, 128, <class 'torch.optim.lr_scheduler.StepLR'>)\n",
      "(<class '__main__.DeepMLP'>, <class 'torch.optim.adam.Adam'>, 0.001, 128, None)\n",
      "(<class '__main__.DeepMLP'>, <class 'torch.optim.adam.Adam'>, 0.001, 64, None)\n",
      "(<class '__main__.DeepMLP'>, <class 'torch.optim.adam.Adam'>, 0.001, 64, <class 'torch.optim.lr_scheduler.StepLR'>)\n",
      "(<class '__main__.DeepMLP'>, <class 'torch.optim.adam.Adam'>, 0.01, 128, None)\n",
      "Top 5 CNN configurations\n",
      "(<class '__main__.CNN'>, <class 'torch.optim.adam.Adam'>, 0.0001, 64, None)\n",
      "(<class '__main__.CNN'>, <class 'torch.optim.adam.Adam'>, 0.0001, 64, <class 'torch.optim.lr_scheduler.StepLR'>)\n",
      "(<class '__main__.CNN'>, <class 'torch.optim.adam.Adam'>, 0.0001, 128, None)\n",
      "(<class '__main__.CNN'>, <class 'torch.optim.adam.Adam'>, 0.0001, 128, <class 'torch.optim.lr_scheduler.StepLR'>)\n",
      "(<class '__main__.CNN'>, <class 'torch.optim.adam.Adam'>, 0.001, 64, None)\n"
     ]
    }
   ],
   "source": [
    "# take the best of each model based on the f1 score\n",
    "results = pd.DataFrame(results)\n",
    "results['model'] = [model.__name__ for model, _, _, _, _ in combinations]\n",
    "\n",
    "best_results = results.groupby('model').max()\n",
    "print(best_results)\n",
    "\n",
    "# get the configuration of the best model of each type\n",
    "best_mlp_config = combinations[results[results['model'] == 'MLP']['f1'].idxmax()]\n",
    "best_deepmlp_config = combinations[results[results['model'] == 'DeepMLP']['f1'].idxmax()]\n",
    "best_cnn_config = combinations[results[results['model'] == 'CNN']['f1'].idxmax()]\n",
    "\n",
    "# save the results\n",
    "results.to_csv('results.csv', index=False)\n",
    "best_results.to_csv('best_results.csv')\n",
    "\n",
    "# print the top 5 best mlps configuration settings\n",
    "print('Top 5 MLP configurations')\n",
    "top_5_mlp = results[results['model'] == 'MLP'].nlargest(5, 'f1')\n",
    "\n",
    "# get the configurations\n",
    "top_5_mlp_configs = [combinations[i] for i in top_5_mlp.index]\n",
    "for i, config in enumerate(top_5_mlp_configs):\n",
    "    print(config)\n",
    "\n",
    "# print the top 5 best deepmlps configuration settings\n",
    "print('Top 5 DeepMLP configurations')\n",
    "top_5_deepmlp = results[results['model'] == 'DeepMLP'].nlargest(5, 'f1')\n",
    "\n",
    "# get the configurations\n",
    "top_5_deepmlp_configs = [combinations[i] for i in top_5_deepmlp.index]\n",
    "for i, config in enumerate(top_5_deepmlp_configs):\n",
    "    print(config)\n",
    "\n",
    "# print the top 5 best cnns configuration settings\n",
    "print('Top 5 CNN configurations')\n",
    "top_5_cnn = results[results['model'] == 'CNN'].nlargest(5, 'f1')\n",
    "\n",
    "# get the configurations\n",
    "top_5_cnn_configs = [combinations[i] for i in top_5_cnn.index]\n",
    "for i, config in enumerate(top_5_cnn_configs):\n",
    "    print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/2000 Train loss: 0.8747 Val loss: 0.8569 Train f1: 0.8104 Val f1: 0.8045\n",
      "Epoch 20/2000 Train loss: 0.8364 Val loss: 0.8193 Train f1: 0.8453 Val f1: 0.8587\n",
      "Epoch 30/2000 Train loss: 0.8252 Val loss: 0.8076 Train f1: 0.8540 Val f1: 0.8911\n",
      "Epoch 40/2000 Train loss: 0.8179 Val loss: 0.8057 Train f1: 0.8734 Val f1: 0.8768\n",
      "Epoch 50/2000 Train loss: 0.8103 Val loss: 0.7951 Train f1: 0.8712 Val f1: 0.9205\n",
      "Epoch 60/2000 Train loss: 0.8087 Val loss: 0.7970 Train f1: 0.8809 Val f1: 0.8982\n",
      "Epoch 70/2000 Train loss: 0.8048 Val loss: 0.7967 Train f1: 0.8826 Val f1: 0.9089\n",
      "Epoch 80/2000 Train loss: 0.7996 Val loss: 0.8026 Train f1: 0.8983 Val f1: 0.8600\n",
      "Epoch 90/2000 Train loss: 0.7969 Val loss: 0.7998 Train f1: 0.9040 Val f1: 0.8787\n",
      "Epoch 100/2000 Train loss: 0.7945 Val loss: 0.8039 Train f1: 0.9111 Val f1: 0.8461\n",
      "Epoch 110/2000 Train loss: 0.7904 Val loss: 0.7986 Train f1: 0.9136 Val f1: 0.8715\n",
      "Epoch 120/2000 Train loss: 0.7916 Val loss: 0.8031 Train f1: 0.9095 Val f1: 0.8588\n",
      "Epoch 130/2000 Train loss: 0.7914 Val loss: 0.7997 Train f1: 0.9109 Val f1: 0.8663\n",
      "Epoch 140/2000 Train loss: 0.7860 Val loss: 0.7986 Train f1: 0.9249 Val f1: 0.8908\n",
      "Epoch 150/2000 Train loss: 0.7860 Val loss: 0.8117 Train f1: 0.9243 Val f1: 0.8331\n",
      "Epoch 160/2000 Train loss: 0.7807 Val loss: 0.7996 Train f1: 0.9347 Val f1: 0.8522\n",
      "Early stopping at epoch 162\n",
      "Best epoch: 62 with val f1: 0.9277\n",
      "Epoch 10/2000 Train loss: 0.9102 Val loss: 0.9009 Train f1: 0.8121 Val f1: 0.7740\n",
      "Epoch 20/2000 Train loss: 0.8433 Val loss: 0.8412 Train f1: 0.8449 Val f1: 0.7972\n",
      "Epoch 30/2000 Train loss: 0.8187 Val loss: 0.8099 Train f1: 0.8664 Val f1: 0.8911\n",
      "Epoch 40/2000 Train loss: 0.8069 Val loss: 0.8178 Train f1: 0.8815 Val f1: 0.8335\n",
      "Epoch 50/2000 Train loss: 0.8041 Val loss: 0.7977 Train f1: 0.8870 Val f1: 0.8948\n",
      "Epoch 60/2000 Train loss: 0.8023 Val loss: 0.8022 Train f1: 0.8938 Val f1: 0.8814\n",
      "Epoch 70/2000 Train loss: 0.7954 Val loss: 0.8160 Train f1: 0.9021 Val f1: 0.8135\n",
      "Epoch 80/2000 Train loss: 0.7972 Val loss: 0.7932 Train f1: 0.9029 Val f1: 0.8705\n",
      "Epoch 90/2000 Train loss: 0.7972 Val loss: 0.8051 Train f1: 0.8992 Val f1: 0.8538\n",
      "Epoch 100/2000 Train loss: 0.7982 Val loss: 0.8091 Train f1: 0.9017 Val f1: 0.8415\n",
      "Epoch 110/2000 Train loss: 0.7952 Val loss: 0.8145 Train f1: 0.8988 Val f1: 0.8108\n",
      "Epoch 120/2000 Train loss: 0.7906 Val loss: 0.7975 Train f1: 0.9138 Val f1: 0.8705\n",
      "Epoch 130/2000 Train loss: 0.7878 Val loss: 0.8158 Train f1: 0.9149 Val f1: 0.8070\n",
      "Epoch 140/2000 Train loss: 0.7904 Val loss: 0.8094 Train f1: 0.9096 Val f1: 0.8392\n",
      "Epoch 150/2000 Train loss: 0.7907 Val loss: 0.7932 Train f1: 0.9118 Val f1: 0.8749\n",
      "Epoch 160/2000 Train loss: 0.7854 Val loss: 0.7986 Train f1: 0.9176 Val f1: 0.8483\n",
      "Epoch 170/2000 Train loss: 0.7875 Val loss: 0.8102 Train f1: 0.9095 Val f1: 0.8631\n",
      "Epoch 180/2000 Train loss: 0.7845 Val loss: 0.7937 Train f1: 0.9238 Val f1: 0.8715\n",
      "Epoch 190/2000 Train loss: 0.7861 Val loss: 0.7842 Train f1: 0.9207 Val f1: 0.9103\n",
      "Epoch 200/2000 Train loss: 0.7806 Val loss: 0.7981 Train f1: 0.9305 Val f1: 0.8600\n",
      "Epoch 210/2000 Train loss: 0.7837 Val loss: 0.8088 Train f1: 0.9196 Val f1: 0.8214\n",
      "Epoch 220/2000 Train loss: 0.7822 Val loss: 0.7999 Train f1: 0.9247 Val f1: 0.8449\n",
      "Epoch 230/2000 Train loss: 0.7908 Val loss: 0.7998 Train f1: 0.9141 Val f1: 0.8669\n",
      "Epoch 240/2000 Train loss: 0.7837 Val loss: 0.7883 Train f1: 0.9249 Val f1: 0.8928\n",
      "Epoch 250/2000 Train loss: 0.7783 Val loss: 0.8053 Train f1: 0.9324 Val f1: 0.8354\n",
      "Epoch 260/2000 Train loss: 0.7770 Val loss: 0.8042 Train f1: 0.9354 Val f1: 0.8530\n",
      "Early stopping at epoch 264\n",
      "Best epoch: 164 with val f1: 0.9103\n",
      "Epoch 10/2000 Train loss: 0.9717 Val loss: 0.9684 Train f1: 0.2907 Val f1: 0.2914\n",
      "Epoch 20/2000 Train loss: 0.9706 Val loss: 0.9673 Train f1: 0.2907 Val f1: 0.2914\n",
      "Epoch 30/2000 Train loss: 0.9705 Val loss: 0.9672 Train f1: 0.2907 Val f1: 0.2914\n",
      "Epoch 40/2000 Train loss: 0.9705 Val loss: 0.9672 Train f1: 0.2907 Val f1: 0.2914\n",
      "Epoch 50/2000 Train loss: 0.9705 Val loss: 0.9672 Train f1: 0.2907 Val f1: 0.2914\n",
      "Epoch 60/2000 Train loss: 0.9705 Val loss: 0.9672 Train f1: 0.2907 Val f1: 0.2914\n",
      "Epoch 70/2000 Train loss: 0.9705 Val loss: 0.9672 Train f1: 0.2907 Val f1: 0.2914\n",
      "Epoch 80/2000 Train loss: 0.9705 Val loss: 0.9672 Train f1: 0.2907 Val f1: 0.2914\n",
      "Epoch 90/2000 Train loss: 0.9705 Val loss: 0.9672 Train f1: 0.2907 Val f1: 0.2914\n",
      "Epoch 100/2000 Train loss: 0.9705 Val loss: 0.9672 Train f1: 0.2907 Val f1: 0.2914\n",
      "Early stopping at epoch 101\n",
      "Best epoch: 1 with val f1: 0.2914\n"
     ]
    }
   ],
   "source": [
    "# evaluating the best models on the test set\n",
    "best_mlp = best_mlp_config[0](INPUT_SIZE, OUTPUT_SIZE).to(DEVICE)\n",
    "best_deepmlp = best_deepmlp_config[0](INPUT_SIZE, OUTPUT_SIZE).to(DEVICE)\n",
    "best_cnn = best_cnn_config[0](INPUT_SIZE, OUTPUT_SIZE).to(DEVICE)\n",
    "\n",
    "data_specs = {\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_val': X_val,\n",
    "    'y_val': y_val\n",
    "}\n",
    "\n",
    "# mlp\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_mlp_config[3], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_mlp_config[3], shuffle=False)\n",
    "\n",
    "optimizer = best_mlp_config[1](best_mlp.parameters(), lr=best_mlp_config[2])\n",
    "if best_mlp_config[4] is not None:\n",
    "    if best_mlp_config[4] == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = best_mlp_config[4](optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    else:\n",
    "        scheduler = best_mlp_config[4](optimizer, step_size=10, gamma=0.1)\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "model_specs = {\n",
    "    'model': best_mlp,\n",
    "    'criterion': nn.CrossEntropyLoss(),\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': scheduler,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader\n",
    "}\n",
    "\n",
    "best_mlp = train_model(model_specs, data_specs, EPOCHS, metric_specs=metric_specs, logging=True, return_model=True)\n",
    "\n",
    "# deepmlp\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_deepmlp_config[3], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_deepmlp_config[3], shuffle=False)\n",
    "\n",
    "optimizer = best_deepmlp_config[1](best_deepmlp.parameters(), lr=best_deepmlp_config[2])\n",
    "if best_deepmlp_config[4] is not None:\n",
    "    if best_deepmlp_config[4] == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = best_deepmlp_config[4](optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    else:\n",
    "        scheduler = best_deepmlp_config[4](optimizer, step_size=10, gamma=0.1)\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "model_specs = {\n",
    "    'model': best_deepmlp,\n",
    "    'criterion': nn.CrossEntropyLoss(),\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': scheduler,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader\n",
    "}\n",
    "\n",
    "best_deepmlp = train_model(model_specs, data_specs, EPOCHS, metric_specs=metric_specs, logging=True, return_model=True)\n",
    "\n",
    "# cnn\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_cnn_config[3], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_cnn_config[3], shuffle=False)\n",
    "\n",
    "optimizer = best_cnn_config[1](best_cnn.parameters(), lr=best_cnn_config[2])\n",
    "if best_cnn_config[4] is not None:\n",
    "    if best_cnn_config[4] == optim.lr_scheduler.ReduceLROnPlateau:\n",
    "        scheduler = best_cnn_config[4](optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    else:\n",
    "        scheduler = best_cnn_config[4](optimizer, step_size=10, gamma=0.1)\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "model_specs = {\n",
    "    'model': best_cnn,\n",
    "    'criterion': nn.CrossEntropyLoss(),\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': scheduler,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader\n",
    "}\n",
    "\n",
    "best_cnn = train_model(model_specs, data_specs, EPOCHS, metric_specs=metric_specs, logging=True, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "Accuracy: 0.9061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.95      0.95       340\n",
      "           2       0.67      0.70      0.69        53\n",
      "           3       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.91       426\n",
      "   macro avg       0.82      0.81      0.82       426\n",
      "weighted avg       0.91      0.91      0.91       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data\n",
    "\n",
    "    Args:\n",
    "    model: torch.nn.Module\n",
    "    X: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "    Returns:\n",
    "    accuracy: float\n",
    "    classification_report: str\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "        y_pred = y_pred.argmax(dim=1).cpu().numpy()\n",
    "        y_true = y.cpu().numpy()\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred)\n",
    "    return accuracy, report\n",
    "\n",
    "# evaluate the best models\n",
    "accuracy_mlp, report_mlp = evaluate_model(best_mlp, X_test, y_test)\n",
    "accuracy_deepmlp, report_deepmlp = evaluate_model(best_deepmlp, X_test, y_test)\n",
    "accuracy_cnn, report_cnn = evaluate_model(best_cnn, X_test, y_test)\n",
    "\n",
    "print('MLP')\n",
    "print(f'Accuracy: {accuracy_mlp:.4f}')\n",
    "print(report_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepMLP\n",
      "Accuracy: 0.9131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.95      0.96       340\n",
      "           2       0.66      0.81      0.73        53\n",
      "           3       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.91       426\n",
      "   macro avg       0.85      0.82      0.83       426\n",
      "weighted avg       0.92      0.91      0.91       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('DeepMLP')\n",
    "print(f'Accuracy: {accuracy_deepmlp:.4f}')\n",
    "print(report_deepmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN\n",
      "Accuracy: 0.7981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      1.00      0.89       340\n",
      "           2       0.00      0.00      0.00        53\n",
      "           3       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.80       426\n",
      "   macro avg       0.27      0.33      0.30       426\n",
      "weighted avg       0.64      0.80      0.71       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('CNN')\n",
    "print(f'Accuracy: {accuracy_cnn:.4f}')\n",
    "print(report_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.3967\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.37      0.51       340\n",
      "           2       0.19      0.53      0.28        53\n",
      "           3       0.12      0.45      0.19        33\n",
      "\n",
      "    accuracy                           0.40       426\n",
      "   macro avg       0.38      0.45      0.33       426\n",
      "weighted avg       0.70      0.40      0.46       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline dummy model\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"uniform\", random_state=0)\n",
    "dummy.fit(X_train.cpu().numpy(), y_train.cpu().numpy())\n",
    "y_pred = dummy.predict(X_test.cpu().numpy())\n",
    "accuracy = accuracy_score(y_test.cpu().numpy(), y_pred)\n",
    "print(f'Baseline accuracy: {accuracy:.4f}')\n",
    "print(classification_report(y_test.cpu().numpy(), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.8803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.94      0.94       340\n",
      "           2       0.58      0.58      0.58        53\n",
      "           3       0.67      0.79      0.72        33\n",
      "\n",
      "    accuracy                           0.88       426\n",
      "   macro avg       0.73      0.77      0.75       426\n",
      "weighted avg       0.88      0.88      0.88       426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=0)\n",
    "logreg.fit(X_train.cpu().numpy(), y_train.cpu().numpy())\n",
    "y_pred = logreg.predict(X_test.cpu().numpy())\n",
    "accuracy = accuracy_score(y_test.cpu().numpy(), y_pred)\n",
    "print(f'Logistic regression accuracy: {accuracy:.4f}')\n",
    "print(classification_report(y_test.cpu().numpy(), y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
